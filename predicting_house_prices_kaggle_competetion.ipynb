{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predicting house prices- kaggle competetion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1dfy2_i5Hq8IWhGIv1fBZZh6auBbhxhpe",
      "authorship_tag": "ABX9TyOkhbk+WNdA9jMUhq0MkUTA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidtaleb/Data-science-/blob/master/predicting_house_prices_kaggle_competetion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0J7MCcmFIQk"
      },
      "source": [
        "#***Prediction of house prices using deep learning- Kaggle competetion bold text***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEiTLjVlLr7a"
      },
      "source": [
        "# install kaggle\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C-r09RiyfAD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkFj5CbhLzRr"
      },
      "source": [
        "# crate a direction of kaggle \n",
        "! mkdir ~/.kaggle/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOGqaOdOL5ky"
      },
      "source": [
        "# copy kaggle.json in kaggle file\n",
        "! cp '/content/drive/My Drive/Kaggle/kaggle.json'  ~/.kaggle/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhkj1Uc_Noa6"
      },
      "source": [
        "# give all rights to  kaggle.json \n",
        "! chmod 600  ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkIEeyK6O_ap",
        "outputId": "4c26d1f0-f6bb-4907-d3a6-fb9d7655a88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!kaggle competitions download -c house-prices-advanced-regression-techniques"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/441k [00:00<?, ?B/s]\n",
            "100% 441k/441k [00:00<00:00, 65.6MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/450k [00:00<?, ?B/s]\n",
            "100% 450k/450k [00:00<00:00, 58.4MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/31.2k [00:00<?, ?B/s]\n",
            "100% 31.2k/31.2k [00:00<00:00, 28.4MB/s]\n",
            "Downloading data_description.txt to /content\n",
            "  0% 0.00/13.1k [00:00<?, ?B/s]\n",
            "100% 13.1k/13.1k [00:00<00:00, 10.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_2exqZTjYOI"
      },
      "source": [
        "import pandas as pd \n",
        "data_train=pd.read_csv('/content/train.csv')\n",
        "data_test=pd.read_csv('/content/test.csv')"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdbJ39pkONo",
        "outputId": "d2b888dc-cbae-4faa-cd92-7aab89f7aa43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# checking for messing values : \n",
        "data_train.isnull().any().sum()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zXSNyZEn3ke"
      },
      "source": [
        "df=data_train.drop(['GarageYrBlt','Id','EnclosedPorch','MoSold',\t'YrSold','MSSubClass','BsmtFinSF2','GarageCars','LowQualFinSF','YearBuilt',\t'YearRemodAdd','BsmtFullBath',\t'BsmtHalfBath',\t'FullBath',\t'HalfBath',\t'BedroomAbvGr',\t'KitchenAbvGr',\t'TotRmsAbvGrd','Fireplaces','OverallQual',\t'OverallCond','3SsnPorch',\t'ScreenPorch',\t'PoolArea',\t'MiscVal','SaleType',\t'SaleCondition','PoolQC',\t'Fence',\t'MiscFeature','GarageCond',\t'PavedDrive','GarageQual','GarageFinish','GarageType','FireplaceQu','Functional','KitchenQual','Heating',\t'HeatingQC',\t'CentralAir',\t'Electrical',\t'BsmtFinType2','ExterQual',\t'ExterCond',\t'Foundation',\t'BsmtQual',\t'BsmtCond',\t'BsmtExposure',\t'BsmtFinType1',\t'RoofStyle',\t'RoofMatl',\t'Exterior1st'\t,'Exterior2nd'\t,'MasVnrType','BldgType','HouseStyle','MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2'], axis=1)"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nuItjrEoeOg"
      },
      "source": [
        "# normalize a data frame \n",
        "dataf=((df-df.min())/(df.max()-df.min()))*1"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlYa9ef51s9V",
        "outputId": "3c740e36-7d04-4043-9092-be8f2f5a6405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "dataf.head()"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.150685</td>\n",
              "      <td>0.033420</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.125089</td>\n",
              "      <td>0.064212</td>\n",
              "      <td>0.140098</td>\n",
              "      <td>0.119780</td>\n",
              "      <td>0.413559</td>\n",
              "      <td>0.259231</td>\n",
              "      <td>0.386460</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111517</td>\n",
              "      <td>0.241078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.202055</td>\n",
              "      <td>0.038795</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.173281</td>\n",
              "      <td>0.121575</td>\n",
              "      <td>0.206547</td>\n",
              "      <td>0.212942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174830</td>\n",
              "      <td>0.324401</td>\n",
              "      <td>0.347725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.203583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.160959</td>\n",
              "      <td>0.046507</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.086109</td>\n",
              "      <td>0.185788</td>\n",
              "      <td>0.150573</td>\n",
              "      <td>0.134465</td>\n",
              "      <td>0.419370</td>\n",
              "      <td>0.273549</td>\n",
              "      <td>0.428773</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076782</td>\n",
              "      <td>0.261908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.133562</td>\n",
              "      <td>0.038561</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.038271</td>\n",
              "      <td>0.231164</td>\n",
              "      <td>0.123732</td>\n",
              "      <td>0.143873</td>\n",
              "      <td>0.366102</td>\n",
              "      <td>0.260550</td>\n",
              "      <td>0.452750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063985</td>\n",
              "      <td>0.145952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.215753</td>\n",
              "      <td>0.060576</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.116052</td>\n",
              "      <td>0.209760</td>\n",
              "      <td>0.187398</td>\n",
              "      <td>0.186095</td>\n",
              "      <td>0.509927</td>\n",
              "      <td>0.351168</td>\n",
              "      <td>0.589563</td>\n",
              "      <td>0.224037</td>\n",
              "      <td>0.153565</td>\n",
              "      <td>0.298709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   LotFrontage   LotArea  MasVnrArea  ...  WoodDeckSF  OpenPorchSF  SalePrice\n",
              "0     0.150685  0.033420     0.12250  ...    0.000000     0.111517   0.241078\n",
              "1     0.202055  0.038795     0.00000  ...    0.347725     0.000000   0.203583\n",
              "2     0.160959  0.046507     0.10125  ...    0.000000     0.076782   0.261908\n",
              "3     0.133562  0.038561     0.00000  ...    0.000000     0.063985   0.145952\n",
              "4     0.215753  0.060576     0.21875  ...    0.224037     0.153565   0.298709\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W5qNtF0S7NX"
      },
      "source": [
        "df=df.fillna(1)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_xQ2kqOTRE5",
        "outputId": "1909feb0-a276-49a4-ac28-ac200d811811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f97a5166a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbIAPtW7LZ5"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzth_k_l6ydv"
      },
      "source": [
        "\n",
        "def create_mlp(dim, regress=True):\n",
        "  # define our MLP network \n",
        "  model = Sequential() \n",
        "  model.add(Dense(25, input_dim=dim, activation=\"relu\",kernel_initializer='glorot_uniform')) \n",
        "  model.add(Dense(7,activation=\"linear\"))\n",
        "  model.add(Dense(3, activation=\"linear\"))\n",
        "  model.add(Dense(1, activation=\"linear\"))\n",
        "\t# return our mode\n",
        "  return model"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amf28poP9lpw"
      },
      "source": [
        "(train, test) = train_test_split(df, test_size=0.2, random_state=42)\n",
        "maxPrice = train[\"SalePrice\"].max()\n",
        "trainY = train[\"SalePrice\"] / maxPrice\n",
        "testY = test[\"SalePrice\"] / maxPrice\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnYeYuAm7C68",
        "outputId": "1a5115fb-5730-4cc8-f1fd-2d58c9d9c6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model = create_mlp(X_train.shape[1], regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 1000)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_126 (Dense)            (None, 25)                325       \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 7)                 182       \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 3)                 24        \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 535\n",
            "Trainable params: 535\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg2uZRXi76Gq",
        "outputId": "517a0a51-cff5-4b67-a56c-a0b41e91d53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model.fit(x=X_train, y=y_train, \n",
        "\tvalidation_data=(X_test, y_test),\n",
        "\tepochs=1000, batch_size=8)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 88.0172 - val_loss: 61.9105\n",
            "Epoch 2/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 34.5526 - val_loss: 30.4655\n",
            "Epoch 3/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 26.5194 - val_loss: 27.8819\n",
            "Epoch 4/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 24.2376 - val_loss: 25.0073\n",
            "Epoch 5/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 21.9639 - val_loss: 22.2123\n",
            "Epoch 6/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 19.8949 - val_loss: 19.9993\n",
            "Epoch 7/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 18.3748 - val_loss: 18.1006\n",
            "Epoch 8/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 17.3678 - val_loss: 17.7682\n",
            "Epoch 9/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.9027 - val_loss: 16.8794\n",
            "Epoch 10/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.5706 - val_loss: 17.9485\n",
            "Epoch 11/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.4152 - val_loss: 16.6949\n",
            "Epoch 12/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.3649 - val_loss: 16.9637\n",
            "Epoch 13/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.2859 - val_loss: 16.5907\n",
            "Epoch 14/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.2552 - val_loss: 16.4129\n",
            "Epoch 15/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.1552 - val_loss: 16.3447\n",
            "Epoch 16/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.0769 - val_loss: 16.5939\n",
            "Epoch 17/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.1127 - val_loss: 16.9120\n",
            "Epoch 18/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.0727 - val_loss: 16.3606\n",
            "Epoch 19/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9945 - val_loss: 16.2773\n",
            "Epoch 20/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.0586 - val_loss: 16.1925\n",
            "Epoch 21/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.0218 - val_loss: 16.6423\n",
            "Epoch 22/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 16.0470 - val_loss: 16.1798\n",
            "Epoch 23/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9361 - val_loss: 16.3922\n",
            "Epoch 24/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9316 - val_loss: 16.2705\n",
            "Epoch 25/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9579 - val_loss: 16.3190\n",
            "Epoch 26/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9467 - val_loss: 16.0814\n",
            "Epoch 27/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9106 - val_loss: 16.2288\n",
            "Epoch 28/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9755 - val_loss: 16.0873\n",
            "Epoch 29/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8681 - val_loss: 16.5385\n",
            "Epoch 30/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.9252 - val_loss: 16.0982\n",
            "Epoch 31/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8539 - val_loss: 16.0466\n",
            "Epoch 32/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8853 - val_loss: 16.1368\n",
            "Epoch 33/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8497 - val_loss: 16.4424\n",
            "Epoch 34/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8416 - val_loss: 16.5848\n",
            "Epoch 35/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8390 - val_loss: 16.0145\n",
            "Epoch 36/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7842 - val_loss: 16.7064\n",
            "Epoch 37/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8571 - val_loss: 16.0402\n",
            "Epoch 38/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8339 - val_loss: 16.1842\n",
            "Epoch 39/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8520 - val_loss: 16.0505\n",
            "Epoch 40/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7717 - val_loss: 15.9606\n",
            "Epoch 41/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8196 - val_loss: 15.9577\n",
            "Epoch 42/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.7645 - val_loss: 15.9524\n",
            "Epoch 43/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8494 - val_loss: 16.2479\n",
            "Epoch 44/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7723 - val_loss: 15.9518\n",
            "Epoch 45/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7981 - val_loss: 16.0535\n",
            "Epoch 46/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7510 - val_loss: 15.9365\n",
            "Epoch 47/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7739 - val_loss: 16.1052\n",
            "Epoch 48/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7694 - val_loss: 16.1617\n",
            "Epoch 49/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8010 - val_loss: 16.0576\n",
            "Epoch 50/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7218 - val_loss: 16.0108\n",
            "Epoch 51/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7423 - val_loss: 15.9833\n",
            "Epoch 52/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7500 - val_loss: 16.2268\n",
            "Epoch 53/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8257 - val_loss: 16.0880\n",
            "Epoch 54/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7136 - val_loss: 16.1506\n",
            "Epoch 55/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7009 - val_loss: 16.0545\n",
            "Epoch 56/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7220 - val_loss: 15.8809\n",
            "Epoch 57/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8213 - val_loss: 16.2065\n",
            "Epoch 58/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7138 - val_loss: 16.3528\n",
            "Epoch 59/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7388 - val_loss: 16.3114\n",
            "Epoch 60/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8363 - val_loss: 15.9275\n",
            "Epoch 61/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6894 - val_loss: 16.1035\n",
            "Epoch 62/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6445 - val_loss: 15.8614\n",
            "Epoch 63/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.8405 - val_loss: 15.8700\n",
            "Epoch 64/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7493 - val_loss: 15.8593\n",
            "Epoch 65/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6754 - val_loss: 16.1216\n",
            "Epoch 66/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6641 - val_loss: 15.8558\n",
            "Epoch 67/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6785 - val_loss: 16.0633\n",
            "Epoch 68/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7128 - val_loss: 16.1073\n",
            "Epoch 69/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7281 - val_loss: 16.2125\n",
            "Epoch 70/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7804 - val_loss: 15.8963\n",
            "Epoch 71/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7538 - val_loss: 16.0997\n",
            "Epoch 72/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6974 - val_loss: 16.2341\n",
            "Epoch 73/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7114 - val_loss: 15.8729\n",
            "Epoch 74/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6863 - val_loss: 16.0213\n",
            "Epoch 75/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7207 - val_loss: 15.8755\n",
            "Epoch 76/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6640 - val_loss: 16.1859\n",
            "Epoch 77/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7032 - val_loss: 17.0783\n",
            "Epoch 78/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6187 - val_loss: 15.9573\n",
            "Epoch 79/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6499 - val_loss: 16.0864\n",
            "Epoch 80/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6464 - val_loss: 15.8543\n",
            "Epoch 81/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6164 - val_loss: 16.7423\n",
            "Epoch 82/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6291 - val_loss: 16.1704\n",
            "Epoch 83/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.7007 - val_loss: 15.8369\n",
            "Epoch 84/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6150 - val_loss: 15.8546\n",
            "Epoch 85/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5909 - val_loss: 15.8007\n",
            "Epoch 86/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5186 - val_loss: 15.8472\n",
            "Epoch 87/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6566 - val_loss: 16.1258\n",
            "Epoch 88/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6240 - val_loss: 16.2583\n",
            "Epoch 89/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5897 - val_loss: 15.8922\n",
            "Epoch 90/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6198 - val_loss: 15.9521\n",
            "Epoch 91/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5824 - val_loss: 15.8389\n",
            "Epoch 92/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5191 - val_loss: 15.8411\n",
            "Epoch 93/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5867 - val_loss: 15.8858\n",
            "Epoch 94/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5528 - val_loss: 16.0076\n",
            "Epoch 95/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5642 - val_loss: 16.0105\n",
            "Epoch 96/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6881 - val_loss: 15.7691\n",
            "Epoch 97/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5524 - val_loss: 15.7808\n",
            "Epoch 98/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5988 - val_loss: 15.9289\n",
            "Epoch 99/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5450 - val_loss: 16.4600\n",
            "Epoch 100/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5469 - val_loss: 15.8051\n",
            "Epoch 101/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6390 - val_loss: 15.7960\n",
            "Epoch 102/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5476 - val_loss: 15.8322\n",
            "Epoch 103/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5976 - val_loss: 16.1113\n",
            "Epoch 104/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6736 - val_loss: 15.7513\n",
            "Epoch 105/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5640 - val_loss: 15.9497\n",
            "Epoch 106/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5353 - val_loss: 15.9897\n",
            "Epoch 107/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5146 - val_loss: 15.7433\n",
            "Epoch 108/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5789 - val_loss: 15.9992\n",
            "Epoch 109/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4859 - val_loss: 16.1431\n",
            "Epoch 110/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.6123 - val_loss: 15.8069\n",
            "Epoch 111/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5086 - val_loss: 15.8349\n",
            "Epoch 112/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5036 - val_loss: 15.9688\n",
            "Epoch 113/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5288 - val_loss: 15.8183\n",
            "Epoch 114/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5167 - val_loss: 15.7350\n",
            "Epoch 115/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.5778 - val_loss: 15.7258\n",
            "Epoch 116/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5439 - val_loss: 16.9161\n",
            "Epoch 117/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5672 - val_loss: 15.7074\n",
            "Epoch 118/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5157 - val_loss: 16.2949\n",
            "Epoch 119/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5463 - val_loss: 15.9023\n",
            "Epoch 120/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5181 - val_loss: 15.8016\n",
            "Epoch 121/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4882 - val_loss: 16.0111\n",
            "Epoch 122/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5176 - val_loss: 15.7292\n",
            "Epoch 123/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4551 - val_loss: 15.9853\n",
            "Epoch 124/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5148 - val_loss: 15.7477\n",
            "Epoch 125/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4925 - val_loss: 15.7648\n",
            "Epoch 126/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4901 - val_loss: 16.0389\n",
            "Epoch 127/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4779 - val_loss: 15.7217\n",
            "Epoch 128/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5538 - val_loss: 15.6773\n",
            "Epoch 129/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3896 - val_loss: 15.7321\n",
            "Epoch 130/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4553 - val_loss: 15.6776\n",
            "Epoch 131/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4362 - val_loss: 15.6717\n",
            "Epoch 132/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5430 - val_loss: 16.0377\n",
            "Epoch 133/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5112 - val_loss: 15.6691\n",
            "Epoch 134/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4585 - val_loss: 16.7282\n",
            "Epoch 135/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4807 - val_loss: 15.9501\n",
            "Epoch 136/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4183 - val_loss: 15.7674\n",
            "Epoch 137/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5055 - val_loss: 16.3220\n",
            "Epoch 138/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4774 - val_loss: 16.2118\n",
            "Epoch 139/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5278 - val_loss: 16.3901\n",
            "Epoch 140/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4653 - val_loss: 15.6547\n",
            "Epoch 141/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4235 - val_loss: 15.6769\n",
            "Epoch 142/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3768 - val_loss: 15.6249\n",
            "Epoch 143/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5357 - val_loss: 15.9914\n",
            "Epoch 144/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4847 - val_loss: 15.7313\n",
            "Epoch 145/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.5795 - val_loss: 15.7789\n",
            "Epoch 146/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5155 - val_loss: 16.1560\n",
            "Epoch 147/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4263 - val_loss: 16.3082\n",
            "Epoch 148/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4006 - val_loss: 15.6208\n",
            "Epoch 149/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5644 - val_loss: 15.6858\n",
            "Epoch 150/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4389 - val_loss: 15.6140\n",
            "Epoch 151/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4740 - val_loss: 15.9989\n",
            "Epoch 152/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4514 - val_loss: 15.6009\n",
            "Epoch 153/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5440 - val_loss: 15.8783\n",
            "Epoch 154/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4769 - val_loss: 15.6099\n",
            "Epoch 155/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3951 - val_loss: 15.8806\n",
            "Epoch 156/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4561 - val_loss: 16.0426\n",
            "Epoch 157/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4171 - val_loss: 15.5936\n",
            "Epoch 158/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4579 - val_loss: 15.6657\n",
            "Epoch 159/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3522 - val_loss: 15.6175\n",
            "Epoch 160/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3844 - val_loss: 15.9030\n",
            "Epoch 161/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5073 - val_loss: 15.6181\n",
            "Epoch 162/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3945 - val_loss: 15.6019\n",
            "Epoch 163/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3471 - val_loss: 15.8442\n",
            "Epoch 164/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3716 - val_loss: 16.6724\n",
            "Epoch 165/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4041 - val_loss: 15.6007\n",
            "Epoch 166/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4733 - val_loss: 15.7343\n",
            "Epoch 167/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3827 - val_loss: 15.5826\n",
            "Epoch 168/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3908 - val_loss: 15.7702\n",
            "Epoch 169/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3896 - val_loss: 15.7049\n",
            "Epoch 170/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3675 - val_loss: 15.6481\n",
            "Epoch 171/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3545 - val_loss: 15.5762\n",
            "Epoch 172/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3308 - val_loss: 15.6038\n",
            "Epoch 173/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3685 - val_loss: 15.5532\n",
            "Epoch 174/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.5055 - val_loss: 15.5783\n",
            "Epoch 175/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3651 - val_loss: 16.0575\n",
            "Epoch 176/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4355 - val_loss: 15.9615\n",
            "Epoch 177/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3916 - val_loss: 15.6677\n",
            "Epoch 178/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4062 - val_loss: 15.5295\n",
            "Epoch 179/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4420 - val_loss: 16.2846\n",
            "Epoch 180/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.5152 - val_loss: 15.5800\n",
            "Epoch 181/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3079 - val_loss: 15.5546\n",
            "Epoch 182/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4330 - val_loss: 15.8777\n",
            "Epoch 183/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4055 - val_loss: 15.5753\n",
            "Epoch 184/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3600 - val_loss: 15.5862\n",
            "Epoch 185/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3951 - val_loss: 15.5518\n",
            "Epoch 186/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3710 - val_loss: 15.5626\n",
            "Epoch 187/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3215 - val_loss: 15.5423\n",
            "Epoch 188/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3631 - val_loss: 17.3759\n",
            "Epoch 189/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4169 - val_loss: 15.5455\n",
            "Epoch 190/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3131 - val_loss: 15.6138\n",
            "Epoch 191/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3343 - val_loss: 15.8040\n",
            "Epoch 192/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3587 - val_loss: 15.6058\n",
            "Epoch 193/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3942 - val_loss: 15.6381\n",
            "Epoch 194/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3123 - val_loss: 16.2070\n",
            "Epoch 195/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3679 - val_loss: 15.5775\n",
            "Epoch 196/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3524 - val_loss: 15.6004\n",
            "Epoch 197/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3699 - val_loss: 15.7856\n",
            "Epoch 198/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.4167 - val_loss: 15.9064\n",
            "Epoch 199/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3466 - val_loss: 15.7247\n",
            "Epoch 200/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3153 - val_loss: 16.7373\n",
            "Epoch 201/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3525 - val_loss: 15.6585\n",
            "Epoch 202/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3172 - val_loss: 15.5733\n",
            "Epoch 203/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3423 - val_loss: 15.5461\n",
            "Epoch 204/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2646 - val_loss: 15.5088\n",
            "Epoch 205/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3220 - val_loss: 15.9614\n",
            "Epoch 206/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3018 - val_loss: 15.5526\n",
            "Epoch 207/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3531 - val_loss: 16.8918\n",
            "Epoch 208/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2924 - val_loss: 15.6758\n",
            "Epoch 209/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4007 - val_loss: 15.5932\n",
            "Epoch 210/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2848 - val_loss: 15.7687\n",
            "Epoch 211/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3397 - val_loss: 15.8620\n",
            "Epoch 212/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2436 - val_loss: 15.5148\n",
            "Epoch 213/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3334 - val_loss: 15.5603\n",
            "Epoch 214/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3375 - val_loss: 15.7385\n",
            "Epoch 215/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2564 - val_loss: 15.8045\n",
            "Epoch 216/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3012 - val_loss: 15.6837\n",
            "Epoch 217/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2822 - val_loss: 15.5597\n",
            "Epoch 218/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3433 - val_loss: 15.8020\n",
            "Epoch 219/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4800 - val_loss: 15.8588\n",
            "Epoch 220/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2270 - val_loss: 15.5052\n",
            "Epoch 221/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3949 - val_loss: 15.7201\n",
            "Epoch 222/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2902 - val_loss: 15.6296\n",
            "Epoch 223/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3470 - val_loss: 15.6251\n",
            "Epoch 224/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3242 - val_loss: 15.9251\n",
            "Epoch 225/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2472 - val_loss: 15.5308\n",
            "Epoch 226/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3104 - val_loss: 15.8126\n",
            "Epoch 227/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3028 - val_loss: 15.6562\n",
            "Epoch 228/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3134 - val_loss: 15.6128\n",
            "Epoch 229/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3087 - val_loss: 15.6816\n",
            "Epoch 230/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2182 - val_loss: 15.5953\n",
            "Epoch 231/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3282 - val_loss: 15.5483\n",
            "Epoch 232/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2698 - val_loss: 15.5543\n",
            "Epoch 233/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4099 - val_loss: 15.7281\n",
            "Epoch 234/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2375 - val_loss: 15.5352\n",
            "Epoch 235/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2845 - val_loss: 15.5951\n",
            "Epoch 236/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2775 - val_loss: 15.6418\n",
            "Epoch 237/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2336 - val_loss: 15.5187\n",
            "Epoch 238/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2870 - val_loss: 15.5772\n",
            "Epoch 239/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2825 - val_loss: 15.6194\n",
            "Epoch 240/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2728 - val_loss: 15.8713\n",
            "Epoch 241/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2129 - val_loss: 16.0159\n",
            "Epoch 242/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2658 - val_loss: 16.0894\n",
            "Epoch 243/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2309 - val_loss: 15.5085\n",
            "Epoch 244/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2324 - val_loss: 16.9182\n",
            "Epoch 245/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2274 - val_loss: 15.9132\n",
            "Epoch 246/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3289 - val_loss: 15.8563\n",
            "Epoch 247/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2350 - val_loss: 15.5217\n",
            "Epoch 248/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2258 - val_loss: 16.2160\n",
            "Epoch 249/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3559 - val_loss: 15.5450\n",
            "Epoch 250/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3013 - val_loss: 15.5219\n",
            "Epoch 251/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3563 - val_loss: 15.8459\n",
            "Epoch 252/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2533 - val_loss: 15.6058\n",
            "Epoch 253/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2022 - val_loss: 16.3465\n",
            "Epoch 254/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3281 - val_loss: 15.5202\n",
            "Epoch 255/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2167 - val_loss: 15.8731\n",
            "Epoch 256/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1931 - val_loss: 15.5250\n",
            "Epoch 257/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2693 - val_loss: 15.5874\n",
            "Epoch 258/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2967 - val_loss: 15.8498\n",
            "Epoch 259/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2936 - val_loss: 16.4335\n",
            "Epoch 260/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2503 - val_loss: 15.8628\n",
            "Epoch 261/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2417 - val_loss: 15.5000\n",
            "Epoch 262/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2360 - val_loss: 15.6465\n",
            "Epoch 263/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2581 - val_loss: 15.6581\n",
            "Epoch 264/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2209 - val_loss: 15.5179\n",
            "Epoch 265/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2284 - val_loss: 15.6394\n",
            "Epoch 266/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2588 - val_loss: 15.5748\n",
            "Epoch 267/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2414 - val_loss: 15.6717\n",
            "Epoch 268/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2227 - val_loss: 15.5184\n",
            "Epoch 269/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3546 - val_loss: 15.7714\n",
            "Epoch 270/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2635 - val_loss: 16.4963\n",
            "Epoch 271/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1819 - val_loss: 15.6014\n",
            "Epoch 272/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3033 - val_loss: 16.6036\n",
            "Epoch 273/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3083 - val_loss: 15.7970\n",
            "Epoch 274/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2241 - val_loss: 15.5047\n",
            "Epoch 275/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2508 - val_loss: 15.5038\n",
            "Epoch 276/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3422 - val_loss: 15.5750\n",
            "Epoch 277/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2698 - val_loss: 15.5134\n",
            "Epoch 278/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2286 - val_loss: 15.6754\n",
            "Epoch 279/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2717 - val_loss: 15.6240\n",
            "Epoch 280/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1912 - val_loss: 15.4858\n",
            "Epoch 281/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1597 - val_loss: 15.5214\n",
            "Epoch 282/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2116 - val_loss: 15.6317\n",
            "Epoch 283/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2873 - val_loss: 15.7370\n",
            "Epoch 284/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2473 - val_loss: 15.9071\n",
            "Epoch 285/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2254 - val_loss: 15.9663\n",
            "Epoch 286/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3366 - val_loss: 15.6098\n",
            "Epoch 287/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1877 - val_loss: 15.7952\n",
            "Epoch 288/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.4071 - val_loss: 15.5055\n",
            "Epoch 289/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1707 - val_loss: 15.5384\n",
            "Epoch 290/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2672 - val_loss: 15.5258\n",
            "Epoch 291/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2060 - val_loss: 15.5310\n",
            "Epoch 292/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2220 - val_loss: 16.1180\n",
            "Epoch 293/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1558 - val_loss: 16.0736\n",
            "Epoch 294/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1879 - val_loss: 15.6757\n",
            "Epoch 295/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2641 - val_loss: 15.5741\n",
            "Epoch 296/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2985 - val_loss: 15.5555\n",
            "Epoch 297/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2273 - val_loss: 15.8563\n",
            "Epoch 298/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2914 - val_loss: 15.5767\n",
            "Epoch 299/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2285 - val_loss: 15.8445\n",
            "Epoch 300/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1586 - val_loss: 15.7525\n",
            "Epoch 301/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1615 - val_loss: 15.5199\n",
            "Epoch 302/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1705 - val_loss: 15.5726\n",
            "Epoch 303/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2001 - val_loss: 15.5452\n",
            "Epoch 304/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3595 - val_loss: 16.4964\n",
            "Epoch 305/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1913 - val_loss: 15.5200\n",
            "Epoch 306/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0802 - val_loss: 15.6660\n",
            "Epoch 307/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2338 - val_loss: 15.9833\n",
            "Epoch 308/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2234 - val_loss: 15.5395\n",
            "Epoch 309/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2333 - val_loss: 15.8237\n",
            "Epoch 310/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1733 - val_loss: 15.4854\n",
            "Epoch 311/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1978 - val_loss: 15.5224\n",
            "Epoch 312/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1831 - val_loss: 16.1495\n",
            "Epoch 313/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1465 - val_loss: 15.5013\n",
            "Epoch 314/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1905 - val_loss: 15.5841\n",
            "Epoch 315/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1815 - val_loss: 15.6998\n",
            "Epoch 316/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1606 - val_loss: 15.6664\n",
            "Epoch 317/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2442 - val_loss: 15.9493\n",
            "Epoch 318/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2054 - val_loss: 15.8740\n",
            "Epoch 319/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1810 - val_loss: 15.7473\n",
            "Epoch 320/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1618 - val_loss: 15.6022\n",
            "Epoch 321/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1855 - val_loss: 15.8193\n",
            "Epoch 322/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1766 - val_loss: 15.7593\n",
            "Epoch 323/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2019 - val_loss: 15.5787\n",
            "Epoch 324/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1774 - val_loss: 16.1922\n",
            "Epoch 325/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2240 - val_loss: 16.4090\n",
            "Epoch 326/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2187 - val_loss: 15.9173\n",
            "Epoch 327/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1711 - val_loss: 15.7647\n",
            "Epoch 328/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0986 - val_loss: 15.7096\n",
            "Epoch 329/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1926 - val_loss: 15.8550\n",
            "Epoch 330/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1844 - val_loss: 15.4964\n",
            "Epoch 331/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2634 - val_loss: 15.8860\n",
            "Epoch 332/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1691 - val_loss: 15.7110\n",
            "Epoch 333/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1967 - val_loss: 16.6175\n",
            "Epoch 334/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3210 - val_loss: 15.4891\n",
            "Epoch 335/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2491 - val_loss: 15.6674\n",
            "Epoch 336/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1720 - val_loss: 15.4905\n",
            "Epoch 337/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1114 - val_loss: 15.5627\n",
            "Epoch 338/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2778 - val_loss: 15.5078\n",
            "Epoch 339/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1624 - val_loss: 15.6449\n",
            "Epoch 340/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.3238 - val_loss: 15.9461\n",
            "Epoch 341/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1710 - val_loss: 15.5815\n",
            "Epoch 342/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1674 - val_loss: 15.5999\n",
            "Epoch 343/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1364 - val_loss: 15.5724\n",
            "Epoch 344/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1744 - val_loss: 15.9511\n",
            "Epoch 345/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2727 - val_loss: 16.1519\n",
            "Epoch 346/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1643 - val_loss: 15.5518\n",
            "Epoch 347/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2052 - val_loss: 15.7588\n",
            "Epoch 348/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1314 - val_loss: 16.3894\n",
            "Epoch 349/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1582 - val_loss: 15.5900\n",
            "Epoch 350/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2503 - val_loss: 15.5998\n",
            "Epoch 351/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1180 - val_loss: 16.0415\n",
            "Epoch 352/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1405 - val_loss: 15.6125\n",
            "Epoch 353/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1644 - val_loss: 15.8654\n",
            "Epoch 354/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2050 - val_loss: 15.5531\n",
            "Epoch 355/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1946 - val_loss: 15.5454\n",
            "Epoch 356/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2468 - val_loss: 15.5637\n",
            "Epoch 357/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2191 - val_loss: 15.5118\n",
            "Epoch 358/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1583 - val_loss: 15.5429\n",
            "Epoch 359/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1637 - val_loss: 15.9594\n",
            "Epoch 360/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1901 - val_loss: 15.5755\n",
            "Epoch 361/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1882 - val_loss: 15.5885\n",
            "Epoch 362/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1562 - val_loss: 15.5088\n",
            "Epoch 363/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2308 - val_loss: 15.5050\n",
            "Epoch 364/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1521 - val_loss: 15.5134\n",
            "Epoch 365/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1194 - val_loss: 15.5389\n",
            "Epoch 366/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2156 - val_loss: 16.0301\n",
            "Epoch 367/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0737 - val_loss: 15.5393\n",
            "Epoch 368/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1491 - val_loss: 15.7223\n",
            "Epoch 369/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2550 - val_loss: 16.0629\n",
            "Epoch 370/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1247 - val_loss: 15.8304\n",
            "Epoch 371/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1508 - val_loss: 15.5774\n",
            "Epoch 372/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1094 - val_loss: 15.7509\n",
            "Epoch 373/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1717 - val_loss: 16.1027\n",
            "Epoch 374/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1801 - val_loss: 15.4821\n",
            "Epoch 375/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2540 - val_loss: 16.1116\n",
            "Epoch 376/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2518 - val_loss: 15.4990\n",
            "Epoch 377/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1663 - val_loss: 16.1109\n",
            "Epoch 378/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0930 - val_loss: 15.5468\n",
            "Epoch 379/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1206 - val_loss: 15.4846\n",
            "Epoch 380/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1571 - val_loss: 15.8608\n",
            "Epoch 381/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0953 - val_loss: 15.5075\n",
            "Epoch 382/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1977 - val_loss: 15.5101\n",
            "Epoch 383/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1506 - val_loss: 15.5115\n",
            "Epoch 384/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1340 - val_loss: 15.6908\n",
            "Epoch 385/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1168 - val_loss: 15.7083\n",
            "Epoch 386/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1008 - val_loss: 16.1164\n",
            "Epoch 387/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1686 - val_loss: 15.5601\n",
            "Epoch 388/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1540 - val_loss: 15.5451\n",
            "Epoch 389/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1266 - val_loss: 15.8237\n",
            "Epoch 390/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1522 - val_loss: 15.8187\n",
            "Epoch 391/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1160 - val_loss: 15.4779\n",
            "Epoch 392/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1518 - val_loss: 15.4778\n",
            "Epoch 393/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1430 - val_loss: 15.5028\n",
            "Epoch 394/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1387 - val_loss: 16.8871\n",
            "Epoch 395/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2322 - val_loss: 15.7435\n",
            "Epoch 396/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0881 - val_loss: 15.4733\n",
            "Epoch 397/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1195 - val_loss: 15.6374\n",
            "Epoch 398/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1664 - val_loss: 16.2459\n",
            "Epoch 399/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0932 - val_loss: 15.6609\n",
            "Epoch 400/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1441 - val_loss: 15.5461\n",
            "Epoch 401/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1331 - val_loss: 15.6839\n",
            "Epoch 402/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1785 - val_loss: 15.6404\n",
            "Epoch 403/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1057 - val_loss: 15.6699\n",
            "Epoch 404/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2187 - val_loss: 15.5048\n",
            "Epoch 405/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1331 - val_loss: 15.5059\n",
            "Epoch 406/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1554 - val_loss: 15.5775\n",
            "Epoch 407/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1252 - val_loss: 15.8181\n",
            "Epoch 408/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.3362 - val_loss: 15.7048\n",
            "Epoch 409/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0745 - val_loss: 16.5043\n",
            "Epoch 410/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0654 - val_loss: 15.9774\n",
            "Epoch 411/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1439 - val_loss: 15.8153\n",
            "Epoch 412/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2744 - val_loss: 16.1775\n",
            "Epoch 413/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0257 - val_loss: 16.5214\n",
            "Epoch 414/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1877 - val_loss: 16.4493\n",
            "Epoch 415/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2763 - val_loss: 15.7155\n",
            "Epoch 416/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0982 - val_loss: 16.4221\n",
            "Epoch 417/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2056 - val_loss: 15.5559\n",
            "Epoch 418/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1118 - val_loss: 15.8983\n",
            "Epoch 419/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1360 - val_loss: 15.5966\n",
            "Epoch 420/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1875 - val_loss: 15.5338\n",
            "Epoch 421/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0976 - val_loss: 15.5695\n",
            "Epoch 422/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1893 - val_loss: 15.7040\n",
            "Epoch 423/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1114 - val_loss: 15.6196\n",
            "Epoch 424/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0614 - val_loss: 15.5318\n",
            "Epoch 425/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1268 - val_loss: 15.5829\n",
            "Epoch 426/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1450 - val_loss: 15.5384\n",
            "Epoch 427/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0375 - val_loss: 17.0489\n",
            "Epoch 428/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1350 - val_loss: 15.8976\n",
            "Epoch 429/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1234 - val_loss: 15.9113\n",
            "Epoch 430/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1215 - val_loss: 15.9660\n",
            "Epoch 431/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1256 - val_loss: 15.5875\n",
            "Epoch 432/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0500 - val_loss: 15.5323\n",
            "Epoch 433/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1009 - val_loss: 16.2230\n",
            "Epoch 434/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1187 - val_loss: 16.0794\n",
            "Epoch 435/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1372 - val_loss: 15.6937\n",
            "Epoch 436/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.2179 - val_loss: 15.6011\n",
            "Epoch 437/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1832 - val_loss: 15.6948\n",
            "Epoch 438/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0707 - val_loss: 15.6961\n",
            "Epoch 439/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0641 - val_loss: 15.6082\n",
            "Epoch 440/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1541 - val_loss: 15.5543\n",
            "Epoch 441/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1361 - val_loss: 16.4867\n",
            "Epoch 442/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1434 - val_loss: 15.7743\n",
            "Epoch 443/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1830 - val_loss: 15.5579\n",
            "Epoch 444/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1317 - val_loss: 15.6910\n",
            "Epoch 445/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0837 - val_loss: 15.8393\n",
            "Epoch 446/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1394 - val_loss: 15.7148\n",
            "Epoch 447/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0684 - val_loss: 15.5222\n",
            "Epoch 448/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1396 - val_loss: 15.8417\n",
            "Epoch 449/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0668 - val_loss: 15.6828\n",
            "Epoch 450/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1476 - val_loss: 15.5786\n",
            "Epoch 451/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1764 - val_loss: 15.5184\n",
            "Epoch 452/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1728 - val_loss: 15.6307\n",
            "Epoch 453/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1351 - val_loss: 16.5075\n",
            "Epoch 454/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0897 - val_loss: 15.5781\n",
            "Epoch 455/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0785 - val_loss: 15.7357\n",
            "Epoch 456/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0787 - val_loss: 15.5796\n",
            "Epoch 457/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1669 - val_loss: 15.7767\n",
            "Epoch 458/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0497 - val_loss: 15.6087\n",
            "Epoch 459/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9253 - val_loss: 16.5826\n",
            "Epoch 460/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0917 - val_loss: 15.5512\n",
            "Epoch 461/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0480 - val_loss: 16.4954\n",
            "Epoch 462/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0791 - val_loss: 15.5457\n",
            "Epoch 463/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0798 - val_loss: 15.5820\n",
            "Epoch 464/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1686 - val_loss: 15.5545\n",
            "Epoch 465/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0246 - val_loss: 15.7546\n",
            "Epoch 466/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1145 - val_loss: 16.4142\n",
            "Epoch 467/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1404 - val_loss: 15.8003\n",
            "Epoch 468/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0693 - val_loss: 15.6025\n",
            "Epoch 469/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1192 - val_loss: 16.2104\n",
            "Epoch 470/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1174 - val_loss: 15.5505\n",
            "Epoch 471/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0437 - val_loss: 15.6633\n",
            "Epoch 472/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1186 - val_loss: 15.9238\n",
            "Epoch 473/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1829 - val_loss: 15.9234\n",
            "Epoch 474/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0911 - val_loss: 15.6069\n",
            "Epoch 475/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1135 - val_loss: 15.6211\n",
            "Epoch 476/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0789 - val_loss: 15.9347\n",
            "Epoch 477/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1151 - val_loss: 15.5816\n",
            "Epoch 478/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1483 - val_loss: 15.5888\n",
            "Epoch 479/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0406 - val_loss: 15.5609\n",
            "Epoch 480/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0991 - val_loss: 15.6898\n",
            "Epoch 481/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1230 - val_loss: 15.5694\n",
            "Epoch 482/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0579 - val_loss: 15.6256\n",
            "Epoch 483/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0703 - val_loss: 15.9112\n",
            "Epoch 484/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1247 - val_loss: 15.5997\n",
            "Epoch 485/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1456 - val_loss: 15.5274\n",
            "Epoch 486/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0911 - val_loss: 16.6729\n",
            "Epoch 487/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1574 - val_loss: 15.6386\n",
            "Epoch 488/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0144 - val_loss: 16.1046\n",
            "Epoch 489/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0598 - val_loss: 15.6758\n",
            "Epoch 490/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0759 - val_loss: 15.8677\n",
            "Epoch 491/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0105 - val_loss: 15.5750\n",
            "Epoch 492/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0772 - val_loss: 15.7817\n",
            "Epoch 493/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9768 - val_loss: 15.5890\n",
            "Epoch 494/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0656 - val_loss: 15.9088\n",
            "Epoch 495/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1797 - val_loss: 15.8040\n",
            "Epoch 496/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1190 - val_loss: 15.6314\n",
            "Epoch 497/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1459 - val_loss: 15.8670\n",
            "Epoch 498/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0353 - val_loss: 15.6612\n",
            "Epoch 499/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0755 - val_loss: 15.6188\n",
            "Epoch 500/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0324 - val_loss: 15.5048\n",
            "Epoch 501/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1140 - val_loss: 16.0507\n",
            "Epoch 502/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0511 - val_loss: 15.7191\n",
            "Epoch 503/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0886 - val_loss: 15.6735\n",
            "Epoch 504/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2282 - val_loss: 16.0768\n",
            "Epoch 505/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1346 - val_loss: 15.6645\n",
            "Epoch 506/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1111 - val_loss: 15.6420\n",
            "Epoch 507/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0786 - val_loss: 15.5849\n",
            "Epoch 508/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0478 - val_loss: 15.7040\n",
            "Epoch 509/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0772 - val_loss: 15.7752\n",
            "Epoch 510/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1006 - val_loss: 16.2136\n",
            "Epoch 511/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0898 - val_loss: 15.5340\n",
            "Epoch 512/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1264 - val_loss: 15.5929\n",
            "Epoch 513/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0332 - val_loss: 15.5406\n",
            "Epoch 514/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0986 - val_loss: 16.1761\n",
            "Epoch 515/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1360 - val_loss: 15.5733\n",
            "Epoch 516/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1825 - val_loss: 16.0183\n",
            "Epoch 517/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0940 - val_loss: 16.0196\n",
            "Epoch 518/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0522 - val_loss: 17.4250\n",
            "Epoch 519/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0670 - val_loss: 15.9311\n",
            "Epoch 520/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1583 - val_loss: 15.5362\n",
            "Epoch 521/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1803 - val_loss: 15.5209\n",
            "Epoch 522/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1087 - val_loss: 16.1022\n",
            "Epoch 523/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0612 - val_loss: 15.8520\n",
            "Epoch 524/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0624 - val_loss: 15.5768\n",
            "Epoch 525/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0747 - val_loss: 16.1530\n",
            "Epoch 526/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0862 - val_loss: 15.5983\n",
            "Epoch 527/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0239 - val_loss: 15.7824\n",
            "Epoch 528/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0475 - val_loss: 16.1468\n",
            "Epoch 529/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1238 - val_loss: 15.5615\n",
            "Epoch 530/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0549 - val_loss: 15.5194\n",
            "Epoch 531/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0298 - val_loss: 15.7255\n",
            "Epoch 532/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0539 - val_loss: 15.7589\n",
            "Epoch 533/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0869 - val_loss: 15.7183\n",
            "Epoch 534/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9995 - val_loss: 15.5277\n",
            "Epoch 535/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.2821 - val_loss: 15.9258\n",
            "Epoch 536/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0346 - val_loss: 15.8374\n",
            "Epoch 537/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0752 - val_loss: 15.5644\n",
            "Epoch 538/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1171 - val_loss: 15.6761\n",
            "Epoch 539/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0117 - val_loss: 16.0474\n",
            "Epoch 540/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0712 - val_loss: 16.3700\n",
            "Epoch 541/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1308 - val_loss: 15.6758\n",
            "Epoch 542/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9956 - val_loss: 16.2206\n",
            "Epoch 543/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0744 - val_loss: 15.8031\n",
            "Epoch 544/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1602 - val_loss: 15.9422\n",
            "Epoch 545/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0886 - val_loss: 15.7736\n",
            "Epoch 546/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0192 - val_loss: 16.7614\n",
            "Epoch 547/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1887 - val_loss: 15.5342\n",
            "Epoch 548/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0500 - val_loss: 15.9188\n",
            "Epoch 549/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0355 - val_loss: 15.6844\n",
            "Epoch 550/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0424 - val_loss: 16.0069\n",
            "Epoch 551/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0698 - val_loss: 15.6656\n",
            "Epoch 552/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1276 - val_loss: 15.6634\n",
            "Epoch 553/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0506 - val_loss: 15.5720\n",
            "Epoch 554/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0720 - val_loss: 15.5412\n",
            "Epoch 555/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0820 - val_loss: 15.5580\n",
            "Epoch 556/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0138 - val_loss: 15.9191\n",
            "Epoch 557/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0361 - val_loss: 15.5465\n",
            "Epoch 558/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0227 - val_loss: 15.5611\n",
            "Epoch 559/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0532 - val_loss: 15.5923\n",
            "Epoch 560/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1060 - val_loss: 15.7831\n",
            "Epoch 561/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0173 - val_loss: 15.6474\n",
            "Epoch 562/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0435 - val_loss: 15.5627\n",
            "Epoch 563/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9655 - val_loss: 15.5900\n",
            "Epoch 564/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0847 - val_loss: 15.5870\n",
            "Epoch 565/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0256 - val_loss: 15.9085\n",
            "Epoch 566/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0242 - val_loss: 15.8383\n",
            "Epoch 567/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0661 - val_loss: 15.5614\n",
            "Epoch 568/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0262 - val_loss: 15.5457\n",
            "Epoch 569/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0430 - val_loss: 15.6082\n",
            "Epoch 570/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0617 - val_loss: 15.7274\n",
            "Epoch 571/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0278 - val_loss: 15.8623\n",
            "Epoch 572/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0752 - val_loss: 15.9365\n",
            "Epoch 573/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0131 - val_loss: 15.7000\n",
            "Epoch 574/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0423 - val_loss: 15.7495\n",
            "Epoch 575/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0681 - val_loss: 15.6079\n",
            "Epoch 576/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0854 - val_loss: 15.6069\n",
            "Epoch 577/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1444 - val_loss: 15.5456\n",
            "Epoch 578/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0666 - val_loss: 15.8297\n",
            "Epoch 579/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9907 - val_loss: 15.8720\n",
            "Epoch 580/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0793 - val_loss: 15.5461\n",
            "Epoch 581/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0011 - val_loss: 15.7822\n",
            "Epoch 582/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0803 - val_loss: 15.5555\n",
            "Epoch 583/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0477 - val_loss: 16.1692\n",
            "Epoch 584/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9944 - val_loss: 15.5717\n",
            "Epoch 585/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0382 - val_loss: 15.6654\n",
            "Epoch 586/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0526 - val_loss: 15.9387\n",
            "Epoch 587/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0755 - val_loss: 15.9588\n",
            "Epoch 588/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9866 - val_loss: 16.0247\n",
            "Epoch 589/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0361 - val_loss: 15.8703\n",
            "Epoch 590/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1163 - val_loss: 15.5889\n",
            "Epoch 591/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0726 - val_loss: 16.0238\n",
            "Epoch 592/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0614 - val_loss: 15.5809\n",
            "Epoch 593/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9709 - val_loss: 15.6859\n",
            "Epoch 594/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0680 - val_loss: 16.3934\n",
            "Epoch 595/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0186 - val_loss: 15.5673\n",
            "Epoch 596/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9452 - val_loss: 15.7291\n",
            "Epoch 597/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0695 - val_loss: 16.8633\n",
            "Epoch 598/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0161 - val_loss: 15.5726\n",
            "Epoch 599/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0921 - val_loss: 15.5854\n",
            "Epoch 600/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0971 - val_loss: 15.5525\n",
            "Epoch 601/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0518 - val_loss: 15.8004\n",
            "Epoch 602/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9446 - val_loss: 15.6298\n",
            "Epoch 603/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0847 - val_loss: 15.9343\n",
            "Epoch 604/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0808 - val_loss: 15.7950\n",
            "Epoch 605/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9998 - val_loss: 16.5225\n",
            "Epoch 606/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1230 - val_loss: 15.6047\n",
            "Epoch 607/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0905 - val_loss: 15.5948\n",
            "Epoch 608/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8856 - val_loss: 15.7956\n",
            "Epoch 609/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9908 - val_loss: 15.5695\n",
            "Epoch 610/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0130 - val_loss: 15.8003\n",
            "Epoch 611/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0680 - val_loss: 15.8288\n",
            "Epoch 612/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9741 - val_loss: 15.6430\n",
            "Epoch 613/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0200 - val_loss: 15.5556\n",
            "Epoch 614/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1091 - val_loss: 15.8223\n",
            "Epoch 615/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0092 - val_loss: 15.5630\n",
            "Epoch 616/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9490 - val_loss: 15.6531\n",
            "Epoch 617/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9939 - val_loss: 15.5386\n",
            "Epoch 618/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0012 - val_loss: 15.5671\n",
            "Epoch 619/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9729 - val_loss: 15.6720\n",
            "Epoch 620/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0592 - val_loss: 15.7544\n",
            "Epoch 621/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0296 - val_loss: 15.6193\n",
            "Epoch 622/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0028 - val_loss: 16.1402\n",
            "Epoch 623/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0071 - val_loss: 15.6456\n",
            "Epoch 624/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9862 - val_loss: 16.1044\n",
            "Epoch 625/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0186 - val_loss: 16.0480\n",
            "Epoch 626/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0587 - val_loss: 15.7027\n",
            "Epoch 627/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0681 - val_loss: 15.7200\n",
            "Epoch 628/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0188 - val_loss: 15.5800\n",
            "Epoch 629/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0619 - val_loss: 15.5812\n",
            "Epoch 630/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0246 - val_loss: 15.6078\n",
            "Epoch 631/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0102 - val_loss: 15.7077\n",
            "Epoch 632/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1486 - val_loss: 15.5805\n",
            "Epoch 633/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1361 - val_loss: 15.9487\n",
            "Epoch 634/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0599 - val_loss: 15.6373\n",
            "Epoch 635/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0186 - val_loss: 15.6138\n",
            "Epoch 636/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0775 - val_loss: 15.6329\n",
            "Epoch 637/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0860 - val_loss: 15.6128\n",
            "Epoch 638/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9891 - val_loss: 15.5714\n",
            "Epoch 639/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9814 - val_loss: 15.5731\n",
            "Epoch 640/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9709 - val_loss: 16.1586\n",
            "Epoch 641/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9755 - val_loss: 15.7581\n",
            "Epoch 642/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9477 - val_loss: 16.1877\n",
            "Epoch 643/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0179 - val_loss: 15.6418\n",
            "Epoch 644/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9634 - val_loss: 15.6052\n",
            "Epoch 645/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0391 - val_loss: 15.8116\n",
            "Epoch 646/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0879 - val_loss: 15.7158\n",
            "Epoch 647/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9809 - val_loss: 15.5668\n",
            "Epoch 648/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9322 - val_loss: 16.0052\n",
            "Epoch 649/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0182 - val_loss: 15.8139\n",
            "Epoch 650/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0096 - val_loss: 15.6381\n",
            "Epoch 651/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0036 - val_loss: 15.5612\n",
            "Epoch 652/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9514 - val_loss: 15.7814\n",
            "Epoch 653/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1142 - val_loss: 15.5676\n",
            "Epoch 654/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9721 - val_loss: 15.5866\n",
            "Epoch 655/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0418 - val_loss: 15.5981\n",
            "Epoch 656/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0252 - val_loss: 15.7669\n",
            "Epoch 657/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0842 - val_loss: 15.6960\n",
            "Epoch 658/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.8919 - val_loss: 16.7777\n",
            "Epoch 659/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0581 - val_loss: 16.2326\n",
            "Epoch 660/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9544 - val_loss: 15.9914\n",
            "Epoch 661/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9304 - val_loss: 16.5060\n",
            "Epoch 662/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8911 - val_loss: 15.5428\n",
            "Epoch 663/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0882 - val_loss: 16.0447\n",
            "Epoch 664/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9427 - val_loss: 16.4537\n",
            "Epoch 665/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1393 - val_loss: 16.4068\n",
            "Epoch 666/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0216 - val_loss: 15.8501\n",
            "Epoch 667/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9365 - val_loss: 15.6132\n",
            "Epoch 668/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9800 - val_loss: 15.9048\n",
            "Epoch 669/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0007 - val_loss: 15.6155\n",
            "Epoch 670/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0758 - val_loss: 15.7291\n",
            "Epoch 671/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0221 - val_loss: 15.9206\n",
            "Epoch 672/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0091 - val_loss: 15.5640\n",
            "Epoch 673/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9957 - val_loss: 15.7925\n",
            "Epoch 674/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9957 - val_loss: 15.8050\n",
            "Epoch 675/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0013 - val_loss: 15.5894\n",
            "Epoch 676/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9973 - val_loss: 15.5593\n",
            "Epoch 677/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9739 - val_loss: 16.2683\n",
            "Epoch 678/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1238 - val_loss: 15.7047\n",
            "Epoch 679/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9499 - val_loss: 16.2074\n",
            "Epoch 680/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9692 - val_loss: 16.3873\n",
            "Epoch 681/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0017 - val_loss: 15.6222\n",
            "Epoch 682/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0903 - val_loss: 15.5536\n",
            "Epoch 683/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9319 - val_loss: 16.3758\n",
            "Epoch 684/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0352 - val_loss: 15.5664\n",
            "Epoch 685/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9510 - val_loss: 15.7558\n",
            "Epoch 686/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9565 - val_loss: 16.4320\n",
            "Epoch 687/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9461 - val_loss: 16.8656\n",
            "Epoch 688/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1309 - val_loss: 15.8946\n",
            "Epoch 689/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0206 - val_loss: 15.8709\n",
            "Epoch 690/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.1102 - val_loss: 15.5909\n",
            "Epoch 691/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9802 - val_loss: 15.6153\n",
            "Epoch 692/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9396 - val_loss: 16.1392\n",
            "Epoch 693/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9522 - val_loss: 15.5493\n",
            "Epoch 694/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0313 - val_loss: 15.6098\n",
            "Epoch 695/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.1151 - val_loss: 15.5580\n",
            "Epoch 696/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9499 - val_loss: 15.5871\n",
            "Epoch 697/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9452 - val_loss: 15.5904\n",
            "Epoch 698/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9035 - val_loss: 15.7564\n",
            "Epoch 699/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0326 - val_loss: 15.7567\n",
            "Epoch 700/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9125 - val_loss: 16.3941\n",
            "Epoch 701/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9545 - val_loss: 15.6530\n",
            "Epoch 702/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9606 - val_loss: 16.4862\n",
            "Epoch 703/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0254 - val_loss: 15.7867\n",
            "Epoch 704/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0020 - val_loss: 15.6592\n",
            "Epoch 705/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9246 - val_loss: 15.5404\n",
            "Epoch 706/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9515 - val_loss: 15.6350\n",
            "Epoch 707/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0106 - val_loss: 15.5572\n",
            "Epoch 708/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9882 - val_loss: 15.9965\n",
            "Epoch 709/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0114 - val_loss: 15.9798\n",
            "Epoch 710/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8995 - val_loss: 15.7291\n",
            "Epoch 711/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0445 - val_loss: 16.1284\n",
            "Epoch 712/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9205 - val_loss: 15.8250\n",
            "Epoch 713/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0387 - val_loss: 15.7765\n",
            "Epoch 714/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9475 - val_loss: 15.6880\n",
            "Epoch 715/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 15.0565 - val_loss: 15.5414\n",
            "Epoch 716/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9075 - val_loss: 15.7529\n",
            "Epoch 717/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0213 - val_loss: 15.7732\n",
            "Epoch 718/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0007 - val_loss: 16.2592\n",
            "Epoch 719/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0240 - val_loss: 15.5596\n",
            "Epoch 720/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9514 - val_loss: 15.5772\n",
            "Epoch 721/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9364 - val_loss: 15.7229\n",
            "Epoch 722/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9433 - val_loss: 15.5412\n",
            "Epoch 723/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0757 - val_loss: 15.5428\n",
            "Epoch 724/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9747 - val_loss: 15.5632\n",
            "Epoch 725/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9501 - val_loss: 15.6172\n",
            "Epoch 726/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0073 - val_loss: 15.5246\n",
            "Epoch 727/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0549 - val_loss: 15.5446\n",
            "Epoch 728/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0665 - val_loss: 16.1879\n",
            "Epoch 729/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0585 - val_loss: 15.5367\n",
            "Epoch 730/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0811 - val_loss: 15.5268\n",
            "Epoch 731/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9532 - val_loss: 15.9976\n",
            "Epoch 732/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0144 - val_loss: 15.5948\n",
            "Epoch 733/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9608 - val_loss: 15.5994\n",
            "Epoch 734/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9809 - val_loss: 15.5981\n",
            "Epoch 735/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9709 - val_loss: 15.5590\n",
            "Epoch 736/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9398 - val_loss: 15.5604\n",
            "Epoch 737/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0594 - val_loss: 15.5791\n",
            "Epoch 738/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9594 - val_loss: 15.7575\n",
            "Epoch 739/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9071 - val_loss: 15.6283\n",
            "Epoch 740/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9748 - val_loss: 15.7594\n",
            "Epoch 741/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0099 - val_loss: 15.5372\n",
            "Epoch 742/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0662 - val_loss: 15.5509\n",
            "Epoch 743/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9406 - val_loss: 15.5642\n",
            "Epoch 744/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9893 - val_loss: 15.8505\n",
            "Epoch 745/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9301 - val_loss: 15.7763\n",
            "Epoch 746/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9515 - val_loss: 15.9006\n",
            "Epoch 747/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9706 - val_loss: 15.9613\n",
            "Epoch 748/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9310 - val_loss: 15.7930\n",
            "Epoch 749/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9479 - val_loss: 15.5382\n",
            "Epoch 750/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0591 - val_loss: 15.8394\n",
            "Epoch 751/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9785 - val_loss: 15.5982\n",
            "Epoch 752/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9434 - val_loss: 15.5267\n",
            "Epoch 753/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9491 - val_loss: 16.0347\n",
            "Epoch 754/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9189 - val_loss: 15.5020\n",
            "Epoch 755/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9881 - val_loss: 15.6233\n",
            "Epoch 756/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0049 - val_loss: 15.5430\n",
            "Epoch 757/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8925 - val_loss: 15.5570\n",
            "Epoch 758/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0135 - val_loss: 15.8241\n",
            "Epoch 759/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9588 - val_loss: 15.7617\n",
            "Epoch 760/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9326 - val_loss: 15.6033\n",
            "Epoch 761/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9627 - val_loss: 15.5257\n",
            "Epoch 762/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9403 - val_loss: 16.2940\n",
            "Epoch 763/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9463 - val_loss: 15.9814\n",
            "Epoch 764/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8902 - val_loss: 15.5750\n",
            "Epoch 765/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9427 - val_loss: 15.5187\n",
            "Epoch 766/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9674 - val_loss: 15.6083\n",
            "Epoch 767/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0328 - val_loss: 15.5404\n",
            "Epoch 768/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9359 - val_loss: 15.5442\n",
            "Epoch 769/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9997 - val_loss: 15.6493\n",
            "Epoch 770/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9797 - val_loss: 16.1389\n",
            "Epoch 771/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0144 - val_loss: 15.5389\n",
            "Epoch 772/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9718 - val_loss: 15.9648\n",
            "Epoch 773/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9151 - val_loss: 15.9834\n",
            "Epoch 774/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9119 - val_loss: 15.5430\n",
            "Epoch 775/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0925 - val_loss: 15.5434\n",
            "Epoch 776/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9515 - val_loss: 16.2162\n",
            "Epoch 777/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9954 - val_loss: 15.5530\n",
            "Epoch 778/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0157 - val_loss: 15.5035\n",
            "Epoch 779/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9127 - val_loss: 16.8896\n",
            "Epoch 780/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9584 - val_loss: 15.5039\n",
            "Epoch 781/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0210 - val_loss: 15.5937\n",
            "Epoch 782/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9908 - val_loss: 15.6138\n",
            "Epoch 783/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8926 - val_loss: 15.6477\n",
            "Epoch 784/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9198 - val_loss: 15.6479\n",
            "Epoch 785/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9767 - val_loss: 15.6367\n",
            "Epoch 786/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0482 - val_loss: 15.7580\n",
            "Epoch 787/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9199 - val_loss: 15.8578\n",
            "Epoch 788/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9644 - val_loss: 16.3099\n",
            "Epoch 789/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9473 - val_loss: 15.5254\n",
            "Epoch 790/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0010 - val_loss: 15.5926\n",
            "Epoch 791/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9663 - val_loss: 15.5148\n",
            "Epoch 792/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0021 - val_loss: 16.3897\n",
            "Epoch 793/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9866 - val_loss: 16.0634\n",
            "Epoch 794/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0247 - val_loss: 16.0418\n",
            "Epoch 795/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9685 - val_loss: 15.5760\n",
            "Epoch 796/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9467 - val_loss: 15.7034\n",
            "Epoch 797/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9598 - val_loss: 15.5315\n",
            "Epoch 798/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0161 - val_loss: 15.7666\n",
            "Epoch 799/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9190 - val_loss: 15.8257\n",
            "Epoch 800/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0579 - val_loss: 16.0229\n",
            "Epoch 801/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9315 - val_loss: 15.5384\n",
            "Epoch 802/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9648 - val_loss: 15.8600\n",
            "Epoch 803/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9621 - val_loss: 15.6225\n",
            "Epoch 804/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9528 - val_loss: 15.7460\n",
            "Epoch 805/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9675 - val_loss: 15.5158\n",
            "Epoch 806/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9663 - val_loss: 16.1752\n",
            "Epoch 807/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0000 - val_loss: 16.7388\n",
            "Epoch 808/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0608 - val_loss: 15.5977\n",
            "Epoch 809/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9419 - val_loss: 15.5698\n",
            "Epoch 810/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9097 - val_loss: 15.5353\n",
            "Epoch 811/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9115 - val_loss: 16.0038\n",
            "Epoch 812/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9979 - val_loss: 15.9952\n",
            "Epoch 813/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9931 - val_loss: 15.7067\n",
            "Epoch 814/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9468 - val_loss: 15.5334\n",
            "Epoch 815/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9983 - val_loss: 15.9740\n",
            "Epoch 816/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9158 - val_loss: 15.8417\n",
            "Epoch 817/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9118 - val_loss: 15.5555\n",
            "Epoch 818/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9210 - val_loss: 15.5124\n",
            "Epoch 819/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9045 - val_loss: 16.1320\n",
            "Epoch 820/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9710 - val_loss: 15.5301\n",
            "Epoch 821/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8874 - val_loss: 16.3467\n",
            "Epoch 822/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9589 - val_loss: 15.5046\n",
            "Epoch 823/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9568 - val_loss: 15.5146\n",
            "Epoch 824/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9241 - val_loss: 15.5297\n",
            "Epoch 825/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9249 - val_loss: 15.5363\n",
            "Epoch 826/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9458 - val_loss: 16.2500\n",
            "Epoch 827/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0539 - val_loss: 15.8431\n",
            "Epoch 828/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9294 - val_loss: 15.6927\n",
            "Epoch 829/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9918 - val_loss: 15.5478\n",
            "Epoch 830/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9419 - val_loss: 15.6819\n",
            "Epoch 831/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0941 - val_loss: 15.5042\n",
            "Epoch 832/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9908 - val_loss: 15.6899\n",
            "Epoch 833/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9727 - val_loss: 15.4926\n",
            "Epoch 834/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9596 - val_loss: 15.4992\n",
            "Epoch 835/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8707 - val_loss: 15.5941\n",
            "Epoch 836/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9279 - val_loss: 16.2198\n",
            "Epoch 837/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0376 - val_loss: 15.8905\n",
            "Epoch 838/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9635 - val_loss: 15.7248\n",
            "Epoch 839/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9656 - val_loss: 15.4728\n",
            "Epoch 840/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0115 - val_loss: 15.6366\n",
            "Epoch 841/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9311 - val_loss: 15.4734\n",
            "Epoch 842/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9317 - val_loss: 15.5248\n",
            "Epoch 843/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9246 - val_loss: 15.5118\n",
            "Epoch 844/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9535 - val_loss: 15.5707\n",
            "Epoch 845/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9036 - val_loss: 15.6025\n",
            "Epoch 846/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0123 - val_loss: 15.8282\n",
            "Epoch 847/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9181 - val_loss: 15.4895\n",
            "Epoch 848/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9372 - val_loss: 15.9872\n",
            "Epoch 849/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9940 - val_loss: 15.8922\n",
            "Epoch 850/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9275 - val_loss: 15.4969\n",
            "Epoch 851/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9929 - val_loss: 15.6397\n",
            "Epoch 852/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0120 - val_loss: 15.5504\n",
            "Epoch 853/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9721 - val_loss: 16.1378\n",
            "Epoch 854/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9631 - val_loss: 16.4853\n",
            "Epoch 855/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9781 - val_loss: 15.6562\n",
            "Epoch 856/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9300 - val_loss: 16.2587\n",
            "Epoch 857/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9884 - val_loss: 15.9990\n",
            "Epoch 858/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8849 - val_loss: 15.6368\n",
            "Epoch 859/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0288 - val_loss: 15.5239\n",
            "Epoch 860/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8887 - val_loss: 16.6359\n",
            "Epoch 861/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9707 - val_loss: 15.6680\n",
            "Epoch 862/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9689 - val_loss: 15.6088\n",
            "Epoch 863/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9370 - val_loss: 15.7711\n",
            "Epoch 864/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9217 - val_loss: 15.6215\n",
            "Epoch 865/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9108 - val_loss: 15.5210\n",
            "Epoch 866/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9564 - val_loss: 15.6498\n",
            "Epoch 867/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8830 - val_loss: 15.4807\n",
            "Epoch 868/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9188 - val_loss: 15.4598\n",
            "Epoch 869/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.8844 - val_loss: 15.4920\n",
            "Epoch 870/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9088 - val_loss: 15.6915\n",
            "Epoch 871/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9335 - val_loss: 15.4764\n",
            "Epoch 872/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9521 - val_loss: 15.4768\n",
            "Epoch 873/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9785 - val_loss: 15.7965\n",
            "Epoch 874/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8852 - val_loss: 16.3325\n",
            "Epoch 875/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9567 - val_loss: 15.5033\n",
            "Epoch 876/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0106 - val_loss: 15.6099\n",
            "Epoch 877/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8712 - val_loss: 15.7809\n",
            "Epoch 878/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9392 - val_loss: 15.5926\n",
            "Epoch 879/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9102 - val_loss: 15.8361\n",
            "Epoch 880/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9003 - val_loss: 15.4812\n",
            "Epoch 881/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9374 - val_loss: 15.6115\n",
            "Epoch 882/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9329 - val_loss: 15.6379\n",
            "Epoch 883/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9397 - val_loss: 15.7325\n",
            "Epoch 884/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9172 - val_loss: 15.4798\n",
            "Epoch 885/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9801 - val_loss: 15.5161\n",
            "Epoch 886/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9641 - val_loss: 15.5083\n",
            "Epoch 887/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9823 - val_loss: 15.4471\n",
            "Epoch 888/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9220 - val_loss: 15.9407\n",
            "Epoch 889/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9484 - val_loss: 15.7888\n",
            "Epoch 890/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9391 - val_loss: 15.4948\n",
            "Epoch 891/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9433 - val_loss: 15.6184\n",
            "Epoch 892/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9372 - val_loss: 15.6198\n",
            "Epoch 893/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0082 - val_loss: 15.4937\n",
            "Epoch 894/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9296 - val_loss: 15.6919\n",
            "Epoch 895/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0319 - val_loss: 15.6599\n",
            "Epoch 896/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9642 - val_loss: 15.4848\n",
            "Epoch 897/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8728 - val_loss: 15.5062\n",
            "Epoch 898/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8327 - val_loss: 15.6572\n",
            "Epoch 899/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9257 - val_loss: 15.7042\n",
            "Epoch 900/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9368 - val_loss: 15.5453\n",
            "Epoch 901/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9269 - val_loss: 15.7878\n",
            "Epoch 902/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9348 - val_loss: 15.4554\n",
            "Epoch 903/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9558 - val_loss: 15.4401\n",
            "Epoch 904/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8732 - val_loss: 15.7569\n",
            "Epoch 905/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9696 - val_loss: 15.5012\n",
            "Epoch 906/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8775 - val_loss: 15.4717\n",
            "Epoch 907/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9865 - val_loss: 15.8396\n",
            "Epoch 908/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9720 - val_loss: 15.5160\n",
            "Epoch 909/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9386 - val_loss: 15.8566\n",
            "Epoch 910/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9344 - val_loss: 15.4725\n",
            "Epoch 911/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9467 - val_loss: 15.4855\n",
            "Epoch 912/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9601 - val_loss: 16.4162\n",
            "Epoch 913/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0183 - val_loss: 15.5014\n",
            "Epoch 914/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9828 - val_loss: 15.4820\n",
            "Epoch 915/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0396 - val_loss: 15.4777\n",
            "Epoch 916/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8718 - val_loss: 15.4936\n",
            "Epoch 917/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9701 - val_loss: 15.7560\n",
            "Epoch 918/1000\n",
            "146/146 [==============================] - 0s 1ms/step - loss: 14.9255 - val_loss: 15.5214\n",
            "Epoch 919/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8744 - val_loss: 15.7689\n",
            "Epoch 920/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0134 - val_loss: 15.4393\n",
            "Epoch 921/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9353 - val_loss: 15.4632\n",
            "Epoch 922/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9259 - val_loss: 15.7713\n",
            "Epoch 923/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8938 - val_loss: 15.7620\n",
            "Epoch 924/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9440 - val_loss: 15.4601\n",
            "Epoch 925/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9113 - val_loss: 15.7465\n",
            "Epoch 926/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9334 - val_loss: 15.4541\n",
            "Epoch 927/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9224 - val_loss: 15.5930\n",
            "Epoch 928/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8897 - val_loss: 15.5512\n",
            "Epoch 929/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8736 - val_loss: 15.5439\n",
            "Epoch 930/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0080 - val_loss: 15.4607\n",
            "Epoch 931/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8990 - val_loss: 15.4958\n",
            "Epoch 932/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9469 - val_loss: 15.4501\n",
            "Epoch 933/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9026 - val_loss: 15.4346\n",
            "Epoch 934/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9344 - val_loss: 15.6762\n",
            "Epoch 935/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9056 - val_loss: 15.4397\n",
            "Epoch 936/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8770 - val_loss: 15.7094\n",
            "Epoch 937/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8755 - val_loss: 15.4523\n",
            "Epoch 938/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8833 - val_loss: 15.5895\n",
            "Epoch 939/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9595 - val_loss: 15.7096\n",
            "Epoch 940/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9394 - val_loss: 15.5422\n",
            "Epoch 941/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8746 - val_loss: 15.6066\n",
            "Epoch 942/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8628 - val_loss: 15.7594\n",
            "Epoch 943/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0585 - val_loss: 15.4711\n",
            "Epoch 944/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9486 - val_loss: 15.5416\n",
            "Epoch 945/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9813 - val_loss: 16.8488\n",
            "Epoch 946/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0158 - val_loss: 15.5303\n",
            "Epoch 947/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0180 - val_loss: 15.4514\n",
            "Epoch 948/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8620 - val_loss: 15.6357\n",
            "Epoch 949/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9179 - val_loss: 15.6837\n",
            "Epoch 950/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8781 - val_loss: 15.5286\n",
            "Epoch 951/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9114 - val_loss: 16.1375\n",
            "Epoch 952/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8738 - val_loss: 15.8442\n",
            "Epoch 953/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9047 - val_loss: 15.5819\n",
            "Epoch 954/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9445 - val_loss: 15.6537\n",
            "Epoch 955/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8816 - val_loss: 15.5137\n",
            "Epoch 956/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9697 - val_loss: 15.6824\n",
            "Epoch 957/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8899 - val_loss: 15.5117\n",
            "Epoch 958/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8535 - val_loss: 15.4492\n",
            "Epoch 959/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9191 - val_loss: 15.5493\n",
            "Epoch 960/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8972 - val_loss: 15.6464\n",
            "Epoch 961/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8707 - val_loss: 15.5879\n",
            "Epoch 962/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9444 - val_loss: 15.4777\n",
            "Epoch 963/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8748 - val_loss: 15.7635\n",
            "Epoch 964/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 15.0400 - val_loss: 15.4721\n",
            "Epoch 965/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9308 - val_loss: 15.8157\n",
            "Epoch 966/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9442 - val_loss: 15.4287\n",
            "Epoch 967/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9759 - val_loss: 15.4440\n",
            "Epoch 968/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9302 - val_loss: 15.4754\n",
            "Epoch 969/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8886 - val_loss: 15.5017\n",
            "Epoch 970/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9170 - val_loss: 15.5377\n",
            "Epoch 971/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8757 - val_loss: 15.4645\n",
            "Epoch 972/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9021 - val_loss: 16.0458\n",
            "Epoch 973/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9041 - val_loss: 16.0356\n",
            "Epoch 974/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9501 - val_loss: 15.5800\n",
            "Epoch 975/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9592 - val_loss: 15.9485\n",
            "Epoch 976/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8876 - val_loss: 16.1993\n",
            "Epoch 977/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8659 - val_loss: 16.1394\n",
            "Epoch 978/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9098 - val_loss: 15.4609\n",
            "Epoch 979/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8930 - val_loss: 15.4886\n",
            "Epoch 980/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9333 - val_loss: 15.4394\n",
            "Epoch 981/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8968 - val_loss: 15.4017\n",
            "Epoch 982/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9158 - val_loss: 15.4342\n",
            "Epoch 983/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9248 - val_loss: 16.0019\n",
            "Epoch 984/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8834 - val_loss: 15.8318\n",
            "Epoch 985/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8120 - val_loss: 15.6704\n",
            "Epoch 986/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9488 - val_loss: 15.6591\n",
            "Epoch 987/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8677 - val_loss: 15.4566\n",
            "Epoch 988/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8807 - val_loss: 15.4176\n",
            "Epoch 989/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9713 - val_loss: 16.7449\n",
            "Epoch 990/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9255 - val_loss: 15.6583\n",
            "Epoch 991/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8828 - val_loss: 16.2783\n",
            "Epoch 992/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8718 - val_loss: 15.5731\n",
            "Epoch 993/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9050 - val_loss: 15.4528\n",
            "Epoch 994/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9330 - val_loss: 15.4524\n",
            "Epoch 995/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8523 - val_loss: 15.5653\n",
            "Epoch 996/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8721 - val_loss: 15.4382\n",
            "Epoch 997/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9449 - val_loss: 15.4421\n",
            "Epoch 998/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.8008 - val_loss: 15.4340\n",
            "Epoch 999/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9289 - val_loss: 15.7278\n",
            "Epoch 1000/1000\n",
            "146/146 [==============================] - 0s 2ms/step - loss: 14.9169 - val_loss: 15.4614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97a370b5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XRm6Lx6T6uD"
      },
      "source": [
        "dft=data_test.drop(['GarageYrBlt','Id','EnclosedPorch','MoSold',\t'YrSold','MSSubClass','BsmtFinSF2','GarageCars','LowQualFinSF','YearBuilt',\t'YearRemodAdd','BsmtFullBath',\t'BsmtHalfBath',\t'FullBath',\t'HalfBath',\t'BedroomAbvGr',\t'KitchenAbvGr',\t'TotRmsAbvGrd','Fireplaces','OverallQual',\t'OverallCond','3SsnPorch',\t'ScreenPorch',\t'PoolArea',\t'MiscVal','SaleType',\t'SaleCondition','PoolQC',\t'Fence',\t'MiscFeature','GarageCond',\t'PavedDrive','GarageQual','GarageFinish','GarageType','FireplaceQu','Functional','KitchenQual','Heating',\t'HeatingQC',\t'CentralAir',\t'Electrical',\t'BsmtFinType2','ExterQual',\t'ExterCond',\t'Foundation',\t'BsmtQual',\t'BsmtCond',\t'BsmtExposure',\t'BsmtFinType1',\t'RoofStyle',\t'RoofMatl',\t'Exterior1st'\t,'Exterior2nd'\t,'MasVnrType','BldgType','HouseStyle','MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2'], axis=1)"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtjNocOyUtAh",
        "outputId": "8d0f20f5-592c-4d62-ea8b-61e44adf52d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "x_test=dft.fillna(1)\n",
        "x_test.head()"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.0</td>\n",
              "      <td>11622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>468.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>882.0</td>\n",
              "      <td>896</td>\n",
              "      <td>0</td>\n",
              "      <td>896</td>\n",
              "      <td>730.0</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81.0</td>\n",
              "      <td>14267</td>\n",
              "      <td>108.0</td>\n",
              "      <td>923.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>1329</td>\n",
              "      <td>0</td>\n",
              "      <td>1329</td>\n",
              "      <td>312.0</td>\n",
              "      <td>393</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>74.0</td>\n",
              "      <td>13830</td>\n",
              "      <td>0.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>928</td>\n",
              "      <td>701</td>\n",
              "      <td>1629</td>\n",
              "      <td>482.0</td>\n",
              "      <td>212</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78.0</td>\n",
              "      <td>9978</td>\n",
              "      <td>20.0</td>\n",
              "      <td>602.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>926.0</td>\n",
              "      <td>926</td>\n",
              "      <td>678</td>\n",
              "      <td>1604</td>\n",
              "      <td>470.0</td>\n",
              "      <td>360</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43.0</td>\n",
              "      <td>5005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>1017.0</td>\n",
              "      <td>1280.0</td>\n",
              "      <td>1280</td>\n",
              "      <td>0</td>\n",
              "      <td>1280</td>\n",
              "      <td>506.0</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   LotFrontage  LotArea  MasVnrArea  ...  GarageArea  WoodDeckSF  OpenPorchSF\n",
              "0         80.0    11622         0.0  ...       730.0         140            0\n",
              "1         81.0    14267       108.0  ...       312.0         393           36\n",
              "2         74.0    13830         0.0  ...       482.0         212           34\n",
              "3         78.0     9978        20.0  ...       470.0         360           36\n",
              "4         43.0     5005         0.0  ...       506.0           0           82\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceje0NcEWJ2a",
        "outputId": "439f7ba7-3276-4902-9b3d-a219ee1a9341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"[INFO] predicting house prices...\")\n",
        "preds = model.predict(x_test)\n"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] predicting house prices...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BNd4MbPWnV6"
      },
      "source": [
        "data_sub1=pd.DataFrame()"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZjNW1PXHm6"
      },
      "source": [
        "data_sub1['Id']=data_test['Id']\n",
        "data_sub1['SalePrice']=preds"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukQy1wIZXZcN",
        "outputId": "e58f8201-3437-4bbe-8ebc-e2875ed757b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_sub1.head()"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1461</td>\n",
              "      <td>145309.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1462</td>\n",
              "      <td>187795.671875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1463</td>\n",
              "      <td>192455.765625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1464</td>\n",
              "      <td>189966.453125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1465</td>\n",
              "      <td>167684.656250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Id      SalePrice\n",
              "0  1461  145309.375000\n",
              "1  1462  187795.671875\n",
              "2  1463  192455.765625\n",
              "3  1464  189966.453125\n",
              "4  1465  167684.656250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU5ZwXb3X4eG"
      },
      "source": [
        "data_sub1.to_csv('/content/sample_data/samplesubmission.csv')"
      ],
      "execution_count": 273,
      "outputs": []
    }
  ]
}